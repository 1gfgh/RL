{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5e29da58",
      "metadata": {
        "id": "5e29da58"
      },
      "source": [
        "## Табличное Q-обучение\n",
        "\n",
        "\n",
        "Одним из наиболее популярных алгоритм обучения на основе временных различий является Q-обучение.\n",
        "\n",
        "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/678cb558a9d59c33ef4810c9618baf34a9577686\">\n",
        "\n",
        "\n",
        "<!-- Раскроем скобки:\n",
        "$$Q(s,a)\\leftarrow (1 - \\alpha) \\times Q(s,a)+\\alpha \\times \\big (r(s)+\\gamma\\max_{a'}Q(a',s')\\big ).$$ -->\n",
        "\n",
        "Для обучения будем использовать среду Taxi-v3. Подробнее про данное окружение можно посмотреть в документации: https://gymnasium.farama.org/environments/toy_text/taxi/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e2ad6ff-2b99-457b-9b1c-68d631b30acd",
      "metadata": {
        "id": "8e2ad6ff-2b99-457b-9b1c-68d631b30acd"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49673813",
      "metadata": {
        "id": "49673813"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf74d77",
      "metadata": {
        "id": "fdf74d77"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Taxi-v3\", render_mode='rgb_array')\n",
        "env.reset()\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14fc4fd5",
      "metadata": {
        "id": "14fc4fd5"
      },
      "outputs": [],
      "source": [
        "def show_progress(rewards_batch, log):\n",
        "    \"\"\"Функция отображения прогресса обучения.\"\"\"\n",
        "    mean_reward = np.mean(rewards_batch)\n",
        "    log.append(mean_reward)\n",
        "\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=[8, 4])\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(log, label='Mean rewards')\n",
        "    plt.legend(loc=4)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff6b545",
      "metadata": {
        "id": "dff6b545"
      },
      "source": [
        "### Задание 1\n",
        "\n",
        "Создайте таблицу из нулей, используя информацию из окружения о количестве состояний и действий (1 балл)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938456a0",
      "metadata": {
        "id": "938456a0"
      },
      "outputs": [],
      "source": [
        "def initialize_q_table(n_observation_space, n_action_space):\n",
        "    # Q = [state][action]\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "    return Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00114313",
      "metadata": {
        "id": "00114313"
      },
      "outputs": [],
      "source": [
        "# Добавить проверку таблицы - размеры, тип.\n",
        "n_states, n_actions = 3, 5\n",
        "Q = initialize_q_table(n_states, n_actions)\n",
        "assert isinstance(Q, np.ndarray) and len(Q.shape) == 2, 'результат должен быть 2D нумпаевским вектором'\n",
        "assert Q.shape[0] == n_states and Q.shape[1] == n_actions, 'проверь размерности: [N состояний][M действий]'\n",
        "assert not Q.any(), 'инициализация должна быть нулями'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df55486b",
      "metadata": {
        "id": "df55486b"
      },
      "source": [
        "### Задание 2\n",
        "\n",
        "Напишите код для epsilon-жадного выбора действия (1 балл):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "159c2559",
      "metadata": {
        "id": "159c2559"
      },
      "outputs": [],
      "source": [
        "def select_action_eps_greedy(Q, state, epsilon):\n",
        "    # выбираем действие, используя eps-greedy исследование среды:\n",
        "    # 1) с вероятностью epsilon выбираем случайное действие,\n",
        "    # 2) иначе выбираем действие жадно\n",
        "    # action =\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b84015fc",
      "metadata": {
        "id": "b84015fc"
      },
      "source": [
        "Напишите код для формулы Q-обновления (1 балл):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "361e30c2",
      "metadata": {
        "id": "361e30c2"
      },
      "outputs": [],
      "source": [
        "def update_Q(Q, s, a, r, next_s, alpha, gamma):\n",
        "    # alpha: learning rate, gamma: discount factor\n",
        "    # вычисли V(next_s) - estimate of optimal future value\n",
        "    # V_ns =\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "\n",
        "    # посчитай TD ошибку\n",
        "    # td_error =\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "\n",
        "    # обновление Q-функции\n",
        "    Q[s, a] += alpha * td_error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b09bd68",
      "metadata": {
        "id": "1b09bd68"
      },
      "source": [
        "Допишите код итерации Q-обучения (2 балла):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e383c953",
      "metadata": {
        "id": "e383c953"
      },
      "outputs": [],
      "source": [
        "# определяем память, в которой будет храниться Q(s,a)\n",
        "Q = initialize_q_table(env.observation_space.n, env.action_space.n)\n",
        "log = []\n",
        "rewards_batch = []\n",
        "\n",
        "# гиперпараметры алгоритма\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.2\n",
        "epsilon_decay = 0.95\n",
        "n_episodes = 10000\n",
        "\n",
        "for i_episode in range(1, n_episodes + 1):\n",
        "    s, _ = env.reset()\n",
        "\n",
        "    r, episode_reward = 0, 0\n",
        "    terminated = False\n",
        "\n",
        "    while not terminated:\n",
        "        # select action eps-greedy\n",
        "        # a = ...\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "\n",
        "        # выполняем действие в среде\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        # Update Q-function\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "\n",
        "        s = next_s\n",
        "        episode_reward += r\n",
        "\n",
        "    rewards_batch.append(episode_reward)\n",
        "    if i_episode % 100 == 0:\n",
        "        show_progress(rewards_batch, log)\n",
        "        rewards_batch = []\n",
        "        print(\n",
        "            f\"Episode: {i_episode}, Reward: {episode_reward}\"\n",
        "            f\", Eps: {epsilon}\"\n",
        "        )\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "assert log[-1] >= -10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66f01e4a",
      "metadata": {
        "id": "66f01e4a"
      },
      "source": [
        "### Интерпретация результатов (1 балл):\n",
        "Если все сделано правильно, то график должен выйти на плато около 0. Значение вознаграждение будет в диапазоне [-5, 10], за счет случайного выбора начальной позиции такси и пассажира. Попробуйте изменить гиперпараметры и сравните результаты."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pqfXuHzImy5d"
      },
      "id": "pqfXuHzImy5d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1cb8c7f9",
      "metadata": {
        "id": "1cb8c7f9"
      },
      "source": [
        "## SARSA\n",
        "\n",
        "Теперь ради интереса попробуем то же самое, но используя метод SARSA.\n",
        "\n",
        "Для начала, добавим функцию обновления Q-функции (1 балл):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5693912f",
      "metadata": {
        "id": "5693912f"
      },
      "outputs": [],
      "source": [
        "def update_Q_SARSA(Q, s, a, r, next_s, alpha, gamma, epsilon):\n",
        "    # выбери следующее действие eps-greedy\n",
        "    # next_a =\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "\n",
        "    # посчитай TD ошибку\n",
        "    # td_error =\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "\n",
        "    # обновление Q-функции\n",
        "    Q[s, a] += alpha * td_error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef6a690",
      "metadata": {
        "id": "4ef6a690"
      },
      "source": [
        "Допишите код итерации SARSA-обучения (2 балла):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34d1d535",
      "metadata": {
        "id": "34d1d535"
      },
      "outputs": [],
      "source": [
        "# определяем память, в которой будет храниться Q(s,a)\n",
        "Q = initialize_q_table(env.observation_space.n, env.action_space.n)\n",
        "log = []\n",
        "rewards_batch = []\n",
        "\n",
        "# гиперпараметры алгоритма\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.2\n",
        "epsilon_decay = 0.95\n",
        "episodes_number = 10000\n",
        "\n",
        "for episode in range(1, episodes_number + 1):\n",
        "    s, _ = env.reset()\n",
        "\n",
        "    r, episode_reward = 0, 0\n",
        "    terminated = False\n",
        "\n",
        "    while not terminated:\n",
        "        # select action eps-greedy\n",
        "        # a = ...\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "\n",
        "        # выполняем действие в среде\n",
        "        next_s, r, terminated, truncated, info = env.step(a)\n",
        "\n",
        "        # Update Q-function with SARSA\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "\n",
        "        s = next_s\n",
        "        episode_reward += r\n",
        "\n",
        "    rewards_batch.append(episode_reward)\n",
        "    if episode % 100 == 0:\n",
        "\n",
        "        show_progress(rewards_batch, log)\n",
        "        rewards_batch = []\n",
        "        print(f\"Episode: {episode}, Reward: {episode_reward}, Eps: {epsilon}\")\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "assert log[-1] >= -10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "094ae59c",
      "metadata": {
        "id": "094ae59c"
      },
      "source": [
        "Сравнивая SARSA и Q-learning, что можно сказать об их кривых обучения? (быстрее-медленнее, обучение стабильнее или нет и тп) (1 балл)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75237165",
      "metadata": {
        "id": "75237165"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}